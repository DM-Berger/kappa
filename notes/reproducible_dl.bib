@misc{bansalSAMSensitivityAttribution2020,
  title = {{{SAM}}: {{The Sensitivity}} of {{Attribution Methods}} to {{Hyperparameters}}},
  shorttitle = {{{SAM}}},
  author = {Bansal, Naman and Agarwal, Chirag and Nguyen, Anh},
  date = {2020-04-12},
  number = {arXiv:2003.08754},
  eprint = {2003.08754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.08754},
  urldate = {2022-07-26},
  abstract = {Attribution methods can provide powerful insights into the reasons for a classifier's decision. We argue that a key desideratum of an explanation method is its robustness to input hyperparameters which are often randomly set or empirically tuned. High sensitivity to arbitrary hyperparameter choices does not only impede reproducibility but also questions the correctness of an explanation and impairs the trust of end-users. In this paper, we provide a thorough empirical study on the sensitivity of existing attribution methods. We found an alarming trend that many methods are highly sensitive to changes in their common hyperparameters e.g. even changing a random seed can yield a different explanation! Interestingly, such sensitivity is not reflected in the average explanation accuracy scores over the dataset as commonly reported in the literature. In addition, explanations generated for robust classifiers (i.e. which are trained to be invariant to pixel-wise perturbations) are surprisingly more robust than those generated for regular classifiers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/LY9KKBRN/Bansal et al. - 2020 - SAM The Sensitivity of Attribution Methods to Hyp.pdf;/Users/derekberger/Zotero/storage/DWAJHL3R/2003.html}
}

@unpublished{benderCanWeightSharing2020,
  title = {Can Weight Sharing Outperform Random Architecture Search? {{An}} Investigation with {{TuNAS}}},
  shorttitle = {Can Weight Sharing Outperform Random Architecture Search?},
  author = {Bender, Gabriel and Liu, Hanxiao and Chen, Bo and Chu, Grace and Cheng, Shuyang and Kindermans, Pieter-Jan and Le, Quoc},
  date = {2020-08-13},
  eprint = {2008.06120},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.06120},
  urldate = {2022-04-12},
  abstract = {Efficient Neural Architecture Search methods based on weight sharing have shown good promise in democratizing Neural Architecture Search for computer vision models. There is, however, an ongoing debate whether these efficient methods are significantly better than random search. Here we perform a thorough comparison between efficient and random search methods on a family of progressively larger and more challenging search spaces for image classification and detection on ImageNet and COCO. While the efficacies of both methods are problem-dependent, our experiments demonstrate that there are large, realistic tasks where efficient search methods can provide substantial gains over random search. In addition, we propose and evaluate techniques which improve the quality of searched architectures and reduce the need for manual hyper-parameter tuning. Source code and experiment data are available at https://github.com/google-research/google-research/tree/master/tunas},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.2.10,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/Y4D7FXZC/Bender et al. - 2020 - Can weight sharing outperform random architecture .pdf;/Users/derekberger/Zotero/storage/XY8RH44H/2008.html}
}

@unpublished{bhojanapalliReproducibilityNeuralNetwork2021,
  title = {On the {{Reproducibility}} of {{Neural Network Predictions}}},
  author = {Bhojanapalli, Srinadh and Wilber, Kimberly and Veit, Andreas and Rawat, Ankit Singh and Kim, Seungyeon and Menon, Aditya and Kumar, Sanjiv},
  date = {2021-02-05},
  eprint = {2102.03349},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.03349},
  urldate = {2022-04-24},
  abstract = {Standard training techniques for neural networks involve multiple sources of randomness, e.g., initialization, mini-batch ordering and in some cases data augmentation. Given that neural networks are heavily over-parameterized in practice, such randomness can cause \{\textbackslash em churn\} -- for the same input, disagreements between predictions of the two models independently trained by the same algorithm, contributing to the `reproducibility challenges' in modern machine learning. In this paper, we study this problem of churn, identify factors that cause it, and propose two simple means of mitigating it. We first demonstrate that churn is indeed an issue, even for standard image classification tasks (CIFAR and ImageNet), and study the role of the different sources of training randomness that cause churn. By analyzing the relationship between churn and prediction confidences, we pursue an approach with two components for churn reduction. First, we propose using \textbackslash emph\{minimum entropy regularizers\} to increase prediction confidences. Second, \textbackslash changes\{we present a novel variant of co-distillation approach\textasciitilde\textbackslash citep\{anil2018large\} to increase model agreement and reduce churn\}. We present empirical results showing the effectiveness of both techniques in reducing churn while improving the accuracy of the underlying model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/NG8UA2YB/Bhojanapalli et al. - 2021 - On the Reproducibility of Neural Network Predictio.pdf;/Users/derekberger/Zotero/storage/AL4MJRSI/2102.html}
}

@article{bouthillierAccountingVarianceMachine2021,
  title = {Accounting for {{Variance}} in {{Machine Learning Benchmarks}}},
  author = {Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Mohammadi Sepahvand, Nazanin and Raff, Edward and Madan, Kanika and Voleti, Vikram and Ebrahimi Kahou, Samira and Michalski, Vincent and Arbel, Tal and Pal, Chris and Varoquaux, Gael and Vincent, Pascal},
  date = {2021-03-15},
  journaltitle = {Proceedings of Machine Learning and Systems},
  volume = {3},
  pages = {747--769},
  url = {https://proceedings.mlsys.org/paper/2021/hash/cfecdb276f634854f3ef915e2e980c31-Abstract.html},
  urldate = {2022-04-20},
  langid = {english},
  file = {/Users/derekberger/Zotero/storage/44Q9AW2J/Bouthillier et al. - 2021 - Accounting for Variance in Machine Learning Benchm.pdf;/Users/derekberger/Zotero/storage/9MGU4PIB/cfecdb276f634854f3ef915e2e980c31-Abstract.html}
}

@inproceedings{bouthillierUnreproducibleResearchReproducible2019,
  title = {Unreproducible {{Research}} Is {{Reproducible}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Bouthillier, Xavier and Laurent, CÃ©sar and Vincent, Pascal},
  date = {2019-05-24},
  pages = {725--734},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/bouthillier19a.html},
  urldate = {2022-04-20},
  abstract = {The apparent contradiction in the title is a wordplay on the different meanings attributed to the word reproducible across different scientific fields. What we imply is that unreproducible findings can be built upon reproducible methods. Without denying the importance of facilitating the reproduction of methods, we deem important to reassert that reproduction of findings is a fundamental step of the scientific inquiry. We argue that the commendable quest towards easy deterministic reproducibility of methods and numerical results should not have us forget the even more important necessity of ensuring the reproducibility of empirical findings and conclusions by properly accounting for essential sources of variations. We provide experiments to exemplify the brittleness of current common practice in the evaluation of models in the field of deep learning, showing that even if the results could be reproduced, a slightly different experiment would not support the findings. We hope to help clarify the distinction between exploratory and empirical research in the field of deep learning and believe more energy should be devoted to proper empirical research in our community. This work is an attempt to promote the use of more rigorous and diversified methodologies. It is not an attempt to impose a new methodology and it is not a critique on the nature of exploratory research.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/derekberger/Zotero/storage/2IDJFAVR/Bouthillier et al. - 2019 - Unreproducible Research is Reproducible.pdf}
}

@misc{choiEmpiricalComparisonsOptimizers2020,
  title = {On {{Empirical Comparisons}} of {{Optimizers}} for {{Deep Learning}}},
  author = {Choi, Dami and Shallue, Christopher J. and Nado, Zachary and Lee, Jaehoon and Maddison, Chris J. and Dahl, George E.},
  date = {2020-06-15},
  number = {arXiv:1910.05446},
  eprint = {1910.05446},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.05446},
  urldate = {2022-05-25},
  abstract = {Selecting an optimizer is a central step in the contemporary deep learning pipeline. In this paper, we demonstrate the sensitivity of optimizer comparisons to the hyperparameter tuning protocol. Our findings suggest that the hyperparameter search space may be the single most important factor explaining the rankings obtained by recent empirical comparisons in the literature. In fact, we show that these results can be contradicted when hyperparameter search spaces are changed. As tuning effort grows without bound, more general optimizers should never underperform the ones they can approximate (i.e., Adam should never perform worse than momentum), but recent attempts to compare optimizers either assume these inclusion relationships are not practically relevant or restrict the hyperparameters in ways that break the inclusions. In our experiments, we find that inclusion relationships between optimizers matter in practice and always predict optimizer comparisons. In particular, we find that the popular adaptive gradient methods never underperform momentum or gradient descent. We also report practical tips around tuning often ignored hyperparameters of adaptive gradient methods and raise concerns about fairly benchmarking optimizers for neural network training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/P75789VD/Choi et al. - 2020 - On Empirical Comparisons of Optimizers for Deep Le.pdf}
}

@unpublished{damourUnderspecificationPresentsChallenges2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  date = {2020-11-24},
  eprint = {2011.03395},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2011.03395},
  urldate = {2022-04-20},
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/8FRYFVF6/D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf;/Users/derekberger/Zotero/storage/HXJRXVQ8/2011.html}
}

@article{davisCalibrationDriftRegression2017,
  title = {Calibration Drift in Regression and Machine Learning Models for Acute Kidney Injury},
  author = {Davis, Sharon E and Lasko, Thomas A and Chen, Guanhua and Siew, Edward D and Matheny, Michael E},
  date = {2017-11-01},
  journaltitle = {Journal of the American Medical Informatics Association},
  shortjournal = {Journal of the American Medical Informatics Association},
  volume = {24},
  number = {6},
  pages = {1052--1061},
  issn = {1067-5027},
  doi = {10.1093/jamia/ocx030},
  url = {https://doi.org/10.1093/jamia/ocx030},
  urldate = {2023-02-10},
  abstract = {Predictive analytics create opportunities to incorporate personalized risk estimates into clinical decision support. Models must be well calibrated to support decision-making, yet calibration deteriorates over time. This study explored the influence of modeling methods on performance drift and connected observed drift with data shifts in the patient population.Using 2003 admissions to Department of Veterans Affairs hospitals nationwide, we developed 7 parallel models for hospital-acquired acute kidney injury using common regression and machine learning methods, validating each over 9 subsequent years.Discrimination was maintained for all models. Calibration declined as all models increasingly overpredicted risk. However, the random forest and neural network models maintained calibration across ranges of probability, capturing more admissions than did the regression models. The magnitude of overprediction increased over time for the regression models while remaining stable and small for the machine learning models. Changes in the rate of acute kidney injury were strongly linked to increasing overprediction, while changes in predictor-outcome associations corresponded with diverging patterns of calibration drift across methods.Efficient and effective updating protocols will be essential for maintaining accuracy of, user confidence in, and safety of personalized risk predictions to support decision-making. Model updating protocols should be tailored to account for variations in calibration drift across methods and respond to periods of rapid performance drift rather than be limited to regularly scheduled annual or biannual intervals.},
  file = {/Users/derekberger/Zotero/storage/MA24XKSM/Davis et al. - 2017 - Calibration drift in regression and machine learni.pdf;/Users/derekberger/Zotero/storage/BFDLR4VA/3096776.html}
}

@article{davisCalibrationDriftRegression2018,
  title = {Calibration {{Drift Among Regression}} and {{Machine Learning Models}} for {{Hospital Mortality}}},
  author = {Davis, Sharon E. and Lasko, Thomas A. and Chen, Guanhua and Matheny, Michael E.},
  date = {2018-04-16},
  journaltitle = {AMIA Annual Symposium Proceedings},
  shortjournal = {AMIA Annu Symp Proc},
  volume = {2017},
  eprint = {29854127},
  eprinttype = {pmid},
  pages = {625--634},
  issn = {1942-597X},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5977580/},
  urldate = {2023-02-10},
  abstract = {Advanced regression and machine learning models can provide personalized risk predictions to support clinical decision-making. We aimed to understand whether modeling methods impact the tendency of calibration to deteriorate as patient populations shift over time, with the goal of informing model updating practices. We developed models for 30-day hospital mortality using seven common regression and machine learning methods. Models were developed on 2006 admissions to Department of Veterans Affairs hospitals and validated on admissions in 2007-2013. All models maintained discrimination. Calibration was stable for the neural network model and declined for all other models. The L-2 penalized logistic regression and random forest models experienced smaller magnitudes of calibration drift than the other regression models. Calibration drift was linked with a changing case mix rather than shifts in predictoroutcome associations or outcome rate. Model updating protocols will need to be tailored to variations in calibration drift across methods.},
  pmcid = {PMC5977580},
  file = {/Users/derekberger/Zotero/storage/SGFXB5LV/Davis et al. - 2018 - Calibration Drift Among Regression and Machine Lea.pdf}
}

@unpublished{hideyReducingModelJitter2022,
  title = {Reducing {{Model Jitter}}: {{Stable Re-training}} of {{Semantic Parsers}} in {{Production Environments}}},
  shorttitle = {Reducing {{Model Jitter}}},
  author = {Hidey, Christopher and Liu, Fei and Goel, Rahul},
  date = {2022-04-10},
  eprint = {2204.04735},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2204.04735},
  urldate = {2022-04-20},
  abstract = {Retraining modern deep learning systems can lead to variations in model performance even when trained using the same data and hyper-parameters by simply using different random seeds. We call this phenomenon model jitter. This issue is often exacerbated in production settings, where models are retrained on noisy data. In this work we tackle the problem of stable retraining with a focus on conversational semantic parsers. We first quantify the model jitter problem by introducing the model agreement metric and showing the variation with dataset noise and model sizes. We then demonstrate the effectiveness of various jitter reduction techniques such as ensembling and distillation. Lastly, we discuss practical trade-offs between such techniques and show that co-distillation provides a sweet spot in terms of jitter reduction for semantic parsing systems with only a modest increase in resource usage.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/derekberger/Zotero/storage/WEJHCKCM/Hidey et al. - 2022 - Reducing Model Jitter Stable Re-training of Seman.pdf;/Users/derekberger/Zotero/storage/BXSR9IHN/2204.html}
}

@article{hosseiniTriedBunchThings2020,
  title = {I Tried a Bunch of Things: {{The}} Dangers of Unexpected Overfitting in Classification of Brain Data},
  shorttitle = {I Tried a Bunch of Things},
  author = {Hosseini, Mahan and Powell, Michael and Collins, John and Callahan-Flintoft, Chloe and Jones, William and Bowman, Howard and Wyble, Brad},
  date = {2020-12-01},
  journaltitle = {Neuroscience \& Biobehavioral Reviews},
  shortjournal = {Neuroscience \& Biobehavioral Reviews},
  volume = {119},
  pages = {456--467},
  issn = {0149-7634},
  doi = {10.1016/j.neubiorev.2020.09.036},
  url = {https://www.sciencedirect.com/science/article/pii/S0149763420305868},
  urldate = {2022-08-02},
  abstract = {Machine learning has enhanced the abilities of neuroscientists to interpret information collected through EEG, fMRI, and MEG data. With these powerful techniques comes the danger of overfitting of hyperparameters which can render results invalid. We refer to this problem as âoverhypingâ and show that it is pernicious despite commonly used precautions. Overhyping occurs when analysis decisions are made after observing analysis outcomes and can produce results that are partially or even completely spurious. It is commonly assumed that cross-validation is an effective protection against overfitting or overhyping, but this is not actually true. In this article, we show that spurious results can be obtained on random data by modifying hyperparameters in seemingly innocuous ways, despite the use of cross-validation. We recommend a number of techniques for limiting overhyping, such as lock boxes, blind analyses, pre-registrations, and nested cross-validation. These techniques, are common in other fields that use machine learning, including computer science and physics. Adopting similar safeguards is critical for ensuring the robustness of machine-learning techniques in the neurosciences.},
  langid = {english},
  keywords = {Analysis,Classification,EEG,Machine learning,Overfitting,Overhyping},
  file = {/Users/derekberger/Zotero/storage/284P9ZI4/Hosseini et al. - 2020 - I tried a bunch of things The dangers of unexpect.pdf;/Users/derekberger/Zotero/storage/E3CSD4PK/S0149763420305868.html}
}

@unpublished{hutchinsonEvaluationGapsMachine2022,
  title = {Evaluation {{Gaps}} in {{Machine Learning Practice}}},
  author = {Hutchinson, Ben and Rostamzadeh, Negar and Greer, Christina and Heller, Katherine and Prabhakaran, Vinodkumar},
  date = {2022-05-11},
  number = {arXiv:2205.05256},
  eprint = {2205.05256},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.05256},
  url = {http://arxiv.org/abs/2205.05256},
  urldate = {2022-05-16},
  abstract = {Forming a reliable judgement of a machine learning (ML) model's appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline's implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/3KWT9I2L/Hutchinson et al. - 2022 - Evaluation Gaps in Machine Learning Practice.pdf;/Users/derekberger/Zotero/storage/XFGQ4VEZ/2205.html}
}

@article{lacsonMachineLearningModel2022,
  title = {Machine {{Learning Model Drift}}: {{Predicting Diagnostic Imaging Follow-Up}} as a {{Case Example}}},
  shorttitle = {Machine {{Learning Model Drift}}},
  author = {Lacson, Ronilda and Eskian, Mahsa and Licaros, Andro and Kapoor, Neena and Khorasani, Ramin},
  date = {2022-10-01},
  journaltitle = {Journal of the American College of Radiology},
  shortjournal = {Journal of the American College of Radiology},
  volume = {19},
  number = {10},
  pages = {1162--1169},
  issn = {1546-1440},
  doi = {10.1016/j.jacr.2022.05.030},
  url = {https://www.sciencedirect.com/science/article/pii/S1546144022005518},
  urldate = {2023-02-10},
  abstract = {Objective Address model drift in a machine learning (ML) model for predicting diagnostic imaging follow-up using data augmentation with more recent data versus retraining new predictive models. Methods This institutional review boardâapproved retrospective study was conducted January 1, 2016, to December 31, 2020, at a large academic institution. A previously trained ML model was trained on 1,000 radiology reports from 2016 (old data). An additional 1,385 randomly selected reports from 2019 to 2020 (new data) were annotated for follow-up recommendations and randomly divided into two sets: training (n~= 900) and testing (n~= 485). Support vector machine and random forest (RF) algorithms were constructed and trained using 900 new data reports plus old data (augmented data, new models) and using only new data (new data, new models). The 2016 baseline model was used as comparator as is and trained with augmented data. Recall was compared with baseline using McNemarâs test. Results Follow-up recommendations were contained in 11.3\% of reports (157 or 1,385). The baseline model retrained with new data had precision~= 0.83 and recall~= 0.54; none significantly different from baseline. A new RF model trained with augmented data had significantly better recall versus the baseline model (0.80 versus 0.66, P~= .04) and comparable precision (0.90 versus 0.86). Discussion ML methods for monitoring follow-up recommendations in radiology reports suffer model drift over time. A newly developed RF model achieved better recall with comparable precision versus simply retraining a previously trained original model with augmented data. Thus, regularly assessing and updating these models is necessary using more recent historical data.},
  langid = {english},
  keywords = {Diagnostic imaging,machine learning,model drift},
  file = {/Users/derekberger/Zotero/storage/CSPSQJ2R/S1546144022005518.html}
}

@unpublished{liRandomSearchReproducibility2019,
  title = {Random {{Search}} and {{Reproducibility}} for {{Neural Architecture Search}}},
  author = {Li, Liam and Talwalkar, Ameet},
  date = {2019-07-30},
  eprint = {1902.07638},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.07638},
  urldate = {2022-04-12},
  abstract = {Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks---PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on multiple runs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/YBKL5TJR/Li and Talwalkar - 2019 - Random Search and Reproducibility for Neural Archi.pdf;/Users/derekberger/Zotero/storage/QKSTPEQB/1902.html}
}

@article{liuReplicabilityReproducibilityDeep2022,
  title = {On the {{Replicability}} and {{Reproducibility}} of {{Deep Learning}} in {{Software Engineering}}},
  author = {Liu, Chao and Gao, Cuiyun and Xia, Xin and Lo, David and Grundy, John and Yang, Xiaohu},
  date = {2022-01-31},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  shortjournal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {31},
  number = {1},
  eprint = {2006.14244},
  eprinttype = {arxiv},
  pages = {1--46},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3477535},
  url = {http://arxiv.org/abs/2006.14244},
  urldate = {2022-04-20},
  abstract = {Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge. Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) replicability - whether the reported experimental result can be approximately reproduced in high probability with the same DL model and the same data; and (2) reproducibility - whether one reported experimental findings can be reproduced by new experiments with the same experimental protocol and DL model, but different sampled real-world data. Unlike traditional machine learning (ML) models, DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process. In this study, we conducted a literature review on 93 DL studies recently published in twenty SE journals or conferences. Our statistics show the urgency of investigating these two factors in SE. Moreover, we re-ran four representative DL models in SE. Experimental results show the importance of replicability and reproducibility, where the reported performance of a DL model could not be replicated for an unstable optimization process. Reproducibility could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data. It is therefore urgent for the SE community to provide a long-lasting link to a replication package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/derekberger/Zotero/storage/G4V4NGZZ/Liu et al. - 2022 - On the Replicability and Reproducibility of Deep L.pdf;/Users/derekberger/Zotero/storage/RXLB8ZRS/2006.html}
}

@unpublished{lucicAreGANsCreated2018,
  title = {Are {{GANs Created Equal}}? {{A Large-Scale Study}}},
  shorttitle = {Are {{GANs Created Equal}}?},
  author = {Lucic, Mario and Kurach, Karol and Michalski, Marcin and Gelly, Sylvain and Bousquet, Olivier},
  date = {2018-10-29},
  eprint = {1711.10337},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.10337},
  urldate = {2022-04-20},
  abstract = {Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \textbackslash cite\{goodfellow2014generative\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/JXZ3WJ36/Lucic et al. - 2018 - Are GANs Created Equal A Large-Scale Study.pdf;/Users/derekberger/Zotero/storage/WEXEUDYN/1711.html}
}

@unpublished{mccoyBERTsFeatherNot2020,
  title = {{{BERTs}} of a Feather Do Not Generalize Together: {{Large}} Variability in Generalization across Models with Similar Test Set Performance},
  shorttitle = {{{BERTs}} of a Feather Do Not Generalize Together},
  author = {McCoy, R. Thomas and Min, Junghyun and Linzen, Tal},
  date = {2020-11-16},
  eprint = {1911.02969},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.02969},
  urldate = {2022-04-20},
  abstract = {If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6\% and 84.8\%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that "the doctor visited the lawyer" does not entail "the lawyer visited the doctor"), accuracy ranged from 0.00\% to 66.2\%. Such variation is likely due to the presence of many local minima that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/derekberger/Zotero/storage/XWNX3B9D/McCoy et al. - 2020 - BERTs of a feather do not generalize together Lar.pdf;/Users/derekberger/Zotero/storage/4M3YY4EA/1911.html}
}

@unpublished{melisStateArtEvaluation2017,
  title = {On the {{State}} of the {{Art}} of {{Evaluation}} in {{Neural Language Models}}},
  author = {Melis, GÃ¡bor and Dyer, Chris and Blunsom, Phil},
  date = {2017-11-20},
  eprint = {1707.05589},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.05589},
  urldate = {2022-04-20},
  abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/derekberger/Zotero/storage/5CZND2C8/Melis et al. - 2017 - On the State of the Art of Evaluation in Neural La.pdf;/Users/derekberger/Zotero/storage/Y7UMIICJ/1707.html}
}

@article{minneEffectChangesTime2012,
  title = {Effect of Changes over Time in the Performance of a Customized {{SAPS-II}} Model on the Quality of Care Assessment},
  author = {Minne, Lilian and Eslami, Saeid and de Keizer, Nicolette and de Jonge, Evert and de Rooij, Sophia E. and Abu-Hanna, Ameen},
  options = {useprefix=true},
  date = {2012-01-01},
  journaltitle = {Intensive Care Medicine},
  shortjournal = {Intensive Care Med},
  volume = {38},
  number = {1},
  pages = {40--46},
  issn = {1432-1238},
  doi = {10.1007/s00134-011-2390-2},
  url = {https://doi.org/10.1007/s00134-011-2390-2},
  urldate = {2023-02-10},
  abstract = {The aim of our study was to explore, using an innovative method, the effect of temporal changes in the mortality prediction performance of an existing model on the quality of care assessment. The prognostic model (rSAPS-II) was a recalibrated Simplified Acute Physiology Score-II model developed for very elderly Intensive Care Unit (ICU) patients.},
  langid = {english},
  keywords = {Elderly patients,Intensive care,Mortality prediction,Predictive performance,Prognostic models,Temporal validation},
  file = {/Users/derekberger/Zotero/storage/PIYHKLCU/Minne et al. - 2012 - Effect of changes over time in the performance of .pdf}
}

@article{moonsRiskPredictionModels2012a,
  title = {Risk Prediction Models: {{II}}. {{External}} Validation, Model Updating, and Impact Assessment},
  shorttitle = {Risk Prediction Models},
  author = {Moons, Karel G. M. and Kengne, Andre Pascal and Grobbee, Diederick E. and Royston, Patrick and Vergouwe, Yvonne and Altman, Douglas G. and Woodward, Mark},
  date = {2012-05-01},
  journaltitle = {Heart},
  shortjournal = {Heart},
  volume = {98},
  number = {9},
  eprint = {22397946},
  eprinttype = {pmid},
  pages = {691--698},
  publisher = {{BMJ Publishing Group Ltd and British Cardiovascular Society}},
  issn = {1355-6037, 1468-201X},
  doi = {10.1136/heartjnl-2011-301247},
  url = {https://heart.bmj.com/content/98/9/691},
  urldate = {2023-02-10},
  abstract = {Clinical prediction models are increasingly used to complement clinical reasoning and decision-making in modern medicine, in general, and in the cardiovascular domain, in particular. To these ends, developed models first and foremost need to provide accurate and (internally and externally) validated estimates of probabilities of specific health conditions or outcomes in the targeted individuals. Subsequently, the adoption of such models by professionals must guide their decision-making, and improve patient outcomes and the cost-effectiveness of care. In the first paper of this series of two companion papers, issues relating to prediction model development, their internal validation, and estimating the added value of a new (bio)marker to existing predictors were discussed. In this second paper, an overview is provided of the consecutive steps for the assessment of the model's predictive performance in new individuals (external validation studies), how to adjust or update existing models to local circumstances or with new predictors, and how to investigate the impact of the uptake of prediction models on clinical decision-making and patient outcomes (impact studies). Each step is illustrated with empirical examples from the cardiovascular field.},
  langid = {english},
  keywords = {clinical hypertension,diabetes,epidemiology,general practice,model impact assessment,model updating,model validation,obesity,Prediction model,prevention,risk prediction},
  file = {/Users/derekberger/Zotero/storage/669VTJW4/Moons et al. - 2012 - Risk prediction models II. External validation, m.pdf}
}

@misc{nadoLargeBatchOptimizer2021,
  title = {A {{Large Batch Optimizer Reality Check}}: {{Traditional}}, {{Generic Optimizers Suffice Across Batch Sizes}}},
  shorttitle = {A {{Large Batch Optimizer Reality Check}}},
  author = {Nado, Zachary and Gilmer, Justin M. and Shallue, Christopher J. and Anil, Rohan and Dahl, George E.},
  date = {2021-06-09},
  number = {arXiv:2102.06356},
  eprint = {2102.06356},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2102.06356},
  urldate = {2022-05-25},
  abstract = {Recently the LARS and LAMB optimizers have been proposed for training neural networks faster using large batch sizes. LARS and LAMB add layer-wise normalization to the update rules of Heavy-ball momentum and Adam, respectively, and have become popular in prominent benchmarks and deep learning libraries. However, without fair comparisons to standard optimizers, it remains an open question whether LARS and LAMB have any benefit over traditional, generic algorithms. In this work we demonstrate that standard optimization algorithms such as Nesterov momentum and Adam can match or exceed the results of LARS and LAMB at large batch sizes. Our results establish new, stronger baselines for future comparisons at these batch sizes and shed light on the difficulties of comparing optimizers for neural network training more generally.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/YNPHC2UR/Nado et al. - 2021 - A Large Batch Optimizer Reality Check Traditional.pdf;/Users/derekberger/Zotero/storage/WYGW4I92/2102.html}
}

@inproceedings{nelsonEvaluatingModelDrift2015,
  title = {Evaluating Model Drift in Machine Learning Algorithms},
  booktitle = {2015 {{IEEE Symposium}} on {{Computational Intelligence}} for {{Security}} and {{Defense Applications}} ({{CISDA}})},
  author = {Nelson, Kevin and Corbin, George and Anania, Mark and Kovacs, Matthew and Tobias, Jeremy and Blowers, Misty},
  date = {2015-05},
  pages = {1--8},
  issn = {2329-6275},
  doi = {10.1109/CISDA.2015.7208643},
  abstract = {Machine learning is rapidly emerging as a valuable technology thanks to its ability to learn patterns from large data sets and solve problems that are impossible to model using conventional programming logic. As machine learning techniques become more mainstream, they are being applied to a wider range of application domains. These algorithms are now trusted to make critical decisions in secure and adversarial environments such as healthcare, fraud detection, and network security, in which mistakes can be incredibly costly. They are also a critical component to most modern autonomous systems. However, the data driven approach utilized by these machine learning methods can prove to be a weakness if the data on which the models rely are corrupted by either nefarious or accidental means. Models that utilize on-line learning or periodic retraining to learn new patterns and account for data distribution changes are particularly susceptible to corruption through model drift. In modeling this type of scenario, specially crafted data points are added to the training set over time to adversely influence the system, inducing model drift which leads to incorrect classifications. Our work is focused on exploring the resistance of various machine learning algorithms to such an approach. In this paper we present an experimental framework designed to measure the susceptibility of anomaly detection algorithms to model drift. We also exhibit our preliminary results using various machine learning algorithms commonly found in intrusion detection research.},
  eventtitle = {2015 {{IEEE Symposium}} on {{Computational Intelligence}} for {{Security}} and {{Defense Applications}} ({{CISDA}})},
  keywords = {adversarial machine learning,Algorithm design and analysis,cyber security,Data models,Hidden Markov models,Image color analysis,intrusion detection systems,Machine learning algorithms,model drift,Security,Training},
  file = {/Users/derekberger/Zotero/storage/FEPHBVD4/7208643.html}
}

@inproceedings{phamProblemsOpportunitiesTraining2020,
  title = {Problems and {{Opportunities}} in {{Training Deep Learning Software Systems}}: {{An Analysis}} of {{Variance}}},
  shorttitle = {Problems and {{Opportunities}} in {{Training Deep Learning Software Systems}}},
  booktitle = {2020 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan},
  date = {2020-09},
  pages = {771--783},
  issn = {2643-1572},
  abstract = {Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation. This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8\%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9\%, the per-class accuracy difference to be up to 52.4\%, and the training time difference to be up to 145.3\%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions. Our researcher and practitioner survey shows that 83.8\% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5Â±3\% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.},
  eventtitle = {2020 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  keywords = {Computational modeling,Debugging,deep learning,Deep learning,Libraries,nondeterminism,Software,Software engineering,Training,variance},
  file = {/Users/derekberger/Zotero/storage/C9WDWKDQ/Pham et al. - 2020 - Problems and Opportunities in Training Deep Learni.pdf;/Users/derekberger/Zotero/storage/IIWYGNCC/9286042.html}
}

@inproceedings{qianAreMyDeep2021,
  title = {Are {{My Deep Learning Systems Fair}}? {{An Empirical Study}} of {{Fixed-Seed Training}}},
  shorttitle = {Are {{My Deep Learning Systems Fair}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Qian, Shangshu and Pham, Viet Hung and Lutellier, Thibaud and Hu, Zeou and Kim, Jungwon and Tan, Lin and Yu, Yaoliang and Chen, Jiahao and Shah, Sameena},
  date = {2021},
  volume = {34},
  pages = {30211--30227},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/fdda6e957f1e5ee2f3b311fe4f145ae1-Abstract.html},
  urldate = {2022-04-20},
  abstract = {Deep learning (DL) systems have been gaining popularity in critical tasks such as credit evaluation and crime prediction. Such systems demand fairness. Recent work shows that DL software implementations introduce variance: identical DL training runs (i.e., identical network, data, configuration, software, and hardware) with a fixed seed produce different models. Such variance could make DL models and networks violate fairness compliance laws, resulting in negative social impact. In this paper, we conduct the first empirical study to quantify the impact of software implementation on the fairness and its variance of DL systems. Our study of 22 mitigation techniques and five baselines reveals up to 12.6\% fairness variance across identical training runs with identical seeds. In addition, most debiasing algorithms have a negative impact on the model such as reducing model accuracy, increasing fairness variance, or increasing accuracy variance. Our literature survey shows that while fairness is gaining popularity in artificial intelligence (AI) related conferences, only 34.4\% of the papers use multiple identical training runs to evaluate their approach, raising concerns about their resultsâ validity. We call for better fairness evaluation and testing protocols to improve fairness and fairness variance of DL systems as well as DL research validity and reproducibility at large.},
  file = {/Users/derekberger/Zotero/storage/334HTQRL/Qian et al. - 2021 - Are My Deep Learning Systems Fair An Empirical St.pdf}
}

@misc{roffeDetectingModelDrift2021,
  title = {Detecting Model Drift Using Polynomial Relations},
  author = {Roffe, Eliran and Ackerman, Samuel and Raz, Orna and Farchi, Eitan},
  date = {2021-12-22},
  number = {arXiv:2110.12506},
  eprint = {2110.12506},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.12506},
  url = {http://arxiv.org/abs/2110.12506},
  urldate = {2023-02-10},
  abstract = {Machine learning models serve critical functions, such as classifying loan applicants as good or bad risks. Each model is trained under the assumption that the data used in training and in the field come from the same underlying unknown distribution. Often, this assumption is broken in practice. It is desirable to identify when this occurs, to minimize the impact on model performance. We suggest a new approach to detecting change in the data distribution by identifying polynomial relations between the data features. We measure the strength of each identified relation using its R-square value. A strong polynomial relation captures a significant trait of the data which should remain stable if the data distribution does not change. We thus use a set of learned strong polynomial relations to identify drift. For a set of polynomial relations that are stronger than a given threshold, we calculate the amount of drift observed for that relation. The amount of drift is measured by calculating the Bayes Factor for the polynomial relation likelihood of the baseline data versus field data. We empirically validate the approach by simulating a range of changes, and identify drift using the Bayes Factor of the polynomial relation likelihood change.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/WITMQ3I7/Roffe et al. - 2021 - Detecting model drift using polynomial relations.pdf;/Users/derekberger/Zotero/storage/DRIWQ8DJ/2110.html}
}

@misc{schmidtDescendingCrowdedValley2021,
  title = {Descending through a {{Crowded Valley}} - {{Benchmarking Deep Learning Optimizers}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  date = {2021-08-10},
  number = {arXiv:2007.01547},
  eprint = {2007.01547},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2007.01547},
  urldate = {2022-05-25},
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of fifteen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing more than \$50,000\$ individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we cannot discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific optimizers and parameter choices that generally lead to competitive results in our experiments: Adam remains a strong contender, with newer methods failing to significantly and consistently outperform it. Our open-sourced results are available as challenging and well-tuned baselines for more meaningful evaluations of novel optimization methods without requiring any further computational efforts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{shamirAntiDistillationImprovingReproducibility2020,
  title = {Anti-{{Distillation}}: {{Improving}} Reproducibility of Deep Networks},
  shorttitle = {Anti-{{Distillation}}},
  author = {Shamir, Gil I. and Coviello, Lorenzo},
  date = {2020-10-19},
  eprint = {2010.09923},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2010.09923},
  urldate = {2022-04-24},
  abstract = {Deep networks have been revolutionary in improving performance of machine learning and artificial intelligence systems. Their high prediction accuracy, however, comes at a price of \textbackslash emph\{model irreproducibility\textbackslash/\} in very high levels that do not occur with classical linear models. Two models, even if they are supposedly identical, with identical architecture and identical trained parameter sets, and that are trained on the same set of training examples, while possibly providing identical average prediction accuracies, may predict very differently on individual, previously unseen, examples. \textbackslash emph\{Prediction differences\textbackslash/\} may be as large as the order of magnitude of the predictions themselves. Ensembles have been shown to somewhat mitigate this behavior, but without an extra push, may not be utilizing their full potential. In this work, a novel approach, \textbackslash emph\{Anti-Distillation\textbackslash/\}, is proposed to address irreproducibility in deep networks, where ensemble models are used to generate predictions. Anti-Distillation forces ensemble components away from one another by techniques like de-correlating their outputs over mini-batches of examples, forcing them to become even more different and more diverse. Doing so enhances the benefit of ensembles, making the final predictions more reproducible. Empirical results demonstrate substantial prediction difference reductions achieved by Anti-Distillation on benchmark and real datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/UWJ8WV8D/Shamir and Coviello - 2020 - Anti-Distillation Improving reproducibility of de.pdf;/Users/derekberger/Zotero/storage/LBDCZT4C/2010.html}
}

@misc{sivaprasadOptimizerBenchmarkingNeeds2020,
  title = {Optimizer {{Benchmarking Needs}} to {{Account}} for {{Hyperparameter Tuning}}},
  author = {Sivaprasad, Prabhu Teja and Mai, Florian and Vogels, Thijs and Jaggi, Martin and Fleuret, FranÃ§ois},
  date = {2020-08-15},
  number = {arXiv:1910.11758},
  eprint = {1910.11758},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1910.11758},
  urldate = {2022-05-25},
  abstract = {The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal problem-specific hyperparameters, and finding these settings may be prohibitively costly for practitioners. In this work, we argue that a fair assessment of optimizers' performance must take the computational cost of hyperparameter tuning into account, i.e., how easy it is to find good hyperparameter configurations using an automatic hyperparameter search. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, our results indicate that Adam is the most practical solution, particularly in low-budget scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/6LFLYN6R/Sivaprasad et al. - 2020 - Optimizer Benchmarking Needs to Account for Hyperp.pdf;/Users/derekberger/Zotero/storage/F2P8UBYK/1910.html}
}

@unpublished{snappSynthesizingIrreproducibilityDeep2021,
  title = {Synthesizing {{Irreproducibility}} in {{Deep Networks}}},
  author = {Snapp, Robert R. and Shamir, Gil I.},
  date = {2021-02-21},
  eprint = {2102.10696},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2102.10696},
  urldate = {2022-04-20},
  abstract = {The success and superior performance of deep networks is spreading their popularity and use to an increasing number of applications. Very recent works, however, demonstrate that modern day deep networks suffer from irreproducibility (also referred to as nondeterminism or underspecification). Two or more models that are identical in architecture, structure, training hyper-parameters, and parameters, and that are trained on exactly the same training data, yield different predictions on individual previously unseen examples. Thus, a model that performs well on controlled test data, may perform in unexpected ways when deployed in the real world, whose data is expected to be similar to the test data. We study simple synthetic models and data to understand the origins of these problems. We show that even with a single nonlinearity and for very simple data and models, irreproducibility occurs. Our study demonstrates the effects of randomness in initialization, training data shuffling window size, and activation functions on prediction irreproducibility, even under very controlled synthetic data. While, as one would expect, randomness in initialization and in shuffling the training examples exacerbates the phenomenon, we show that model complexity and the choice of nonlinearity also play significant roles in making deep models irreproducible.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/derekberger/Zotero/storage/4RW25KR7/Snapp and Shamir - 2021 - Synthesizing Irreproducibility in Deep Networks.pdf;/Users/derekberger/Zotero/storage/ZYU3RWQ5/2102.html}
}

@article{varoquauxMachineLearningMedical2022,
  title = {Machine Learning for Medical Imaging: Methodological Failures and Recommendations for the Future},
  shorttitle = {Machine Learning for Medical Imaging},
  author = {Varoquaux, GaÃ«l and Cheplygina, Veronika},
  date = {2022-04-12},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume = {5},
  number = {1},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-022-00592-y},
  url = {https://www.nature.com/articles/s41746-022-00592-y},
  urldate = {2022-04-20},
  abstract = {Research in computer analysis of medical images bears many promises to improve patientsâ health. However, a number of systematic challenges are slowing down the progress of the field, from limitations of the data, such as biases, to research incentives, such as optimizing for publication. In this paper we review roadblocks to developing and assessing methods. Building our analysis on evidence from the literature and data challenges, we show that at every step, potential biases can creep in. On a positive note, we also discuss on-going efforts to counteract these problems. Finally we provide recommendations on how to further address these problems in the future.},
  issue = {1},
  langid = {english},
  keywords = {Computer science,Medical research,Research data},
  file = {/Users/derekberger/Zotero/storage/6Z7F9H6F/Varoquaux and Cheplygina - 2022 - Machine learning for medical imaging methodologic.pdf;/Users/derekberger/Zotero/storage/Z93CTTCU/s41746-022-00592-y.html}
}

@misc{xiongHowMuchProgress2020,
  title = {How Much Progress Have We Made in Neural Network Training? {{A New Evaluation Protocol}} for {{Benchmarking Optimizers}}},
  shorttitle = {How Much Progress Have We Made in Neural Network Training?},
  author = {Xiong, Yuanhao and Liu, Xuanqing and Lan, Li-Cheng and You, Yang and Si, Si and Hsieh, Cho-Jui},
  date = {2020-10-19},
  number = {arXiv:2010.09889},
  eprint = {2010.09889},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.09889},
  urldate = {2022-05-25},
  abstract = {Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which over-emphasizes the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy. A human study is conducted to show that our evaluation protocol matches human tuning behavior better than the random search. For data-addition training, we propose a new protocol for assessing the hyperparameter sensitivity to data shift. We then apply the proposed benchmarking framework to 7 optimizers and various tasks, including computer vision, natural language processing, reinforcement learning, and graph mining. Our results show that there is no clear winner across all the tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@unpublished{zhuangRandomnessNeuralNetwork2021,
  title = {Randomness {{In Neural Network Training}}: {{Characterizing The Impact}} of {{Tooling}}},
  shorttitle = {Randomness {{In Neural Network Training}}},
  author = {Zhuang, Donglin and Zhang, Xingyao and Song, Shuaiwen Leon and Hooker, Sara},
  date = {2021-06-22},
  eprint = {2106.11872},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2106.11872},
  urldate = {2022-04-20},
  abstract = {The quest for determinism in machine learning has disproportionately focused on characterizing the impact of noise introduced by algorithmic design choices. In this work, we address a less well understood and studied question: how does our choice of tooling introduce randomness to deep neural network training. We conduct large scale experiments across different types of hardware, accelerators, state of art networks, and open-source datasets, to characterize how tooling choices contribute to the level of non-determinism in a system, the impact of said non-determinism, and the cost of eliminating different sources of noise. Our findings are surprising, and suggest that the impact of non-determinism in nuanced. While top-line metrics such as top-1 accuracy are not noticeably impacted, model performance on certain parts of the data distribution is far more sensitive to the introduction of randomness. Our results suggest that deterministic tooling is critical for AI safety. However, we also find that the cost of ensuring determinism varies dramatically between neural network architectures and hardware types, e.g., with overhead up to \$746\textbackslash\%\$, \$241\textbackslash\%\$, and \$196\textbackslash\%\$ on a spectrum of widely used GPU accelerator architectures, relative to non-deterministic training. The source code used in this paper is available at https://github.com/usyd-fsalab/NeuralNetworkRandomness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/derekberger/Zotero/storage/BU5CNN3M/Zhuang et al. - 2021 - Randomness In Neural Network Training Characteriz.pdf;/Users/derekberger/Zotero/storage/95R3PZEW/2106.html}
}

@unpublished{zintgrafVisualizingDeepNeural2017,
  title = {Visualizing {{Deep Neural Network Decisions}}: {{Prediction Difference Analysis}}},
  shorttitle = {Visualizing {{Deep Neural Network Decisions}}},
  author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
  date = {2017-02-15},
  eprint = {1702.04595},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1702.04595},
  urldate = {2022-04-20},
  abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/derekberger/Zotero/storage/F23DZ4Z7/Zintgraf et al. - 2017 - Visualizing Deep Neural Network Decisions Predict.pdf;/Users/derekberger/Zotero/storage/PCUWN6VK/1702.html}
}
